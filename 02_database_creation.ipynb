{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcec673",
   "metadata": {},
   "source": [
    "# Chroma Database Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c4587",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df9b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonioparragaleo/miniconda3/envs/RAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "import langchain_text_splitters\n",
    "from langchain_community.document_loaders import PyPDFLoader, pdf\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import unicodedata\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import uuid\n",
    "# from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "import pickle as pkl\n",
    "import requests\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4ea84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_google.txt') as f:\n",
    "    \n",
    "    api_key = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9cd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = api_key['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d295dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\",temperature=0,max_output_tokens=1024) # gemma-3-27b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a4892",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7558ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"info_articles_main.pkl\",\"rb\") as f:\n",
    "    info_articles_main = pkl.load(f)\n",
    "with open(\"info_articles_ref_final.pkl\",\"rb\") as f:\n",
    "    info_articles_ref = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b641d8",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef787c",
   "metadata": {},
   "source": [
    "### Split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e689c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]  # smart splitting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fa4e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_articles_final = info_articles_main + info_articles_ref\n",
    "len(info_articles_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2f19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_splitted = []\n",
    "\n",
    "for j in info_articles_final:\n",
    "\n",
    "    for key, value in j.items():\n",
    "    \n",
    "        if key in ['Abstract', 'Introduction', 'Methods', 'Results', 'Discussion', 'Conclusion',] and value:\n",
    "\n",
    "            if len(value) > 1200:\n",
    "                chunks = splitter.split_text(value)\n",
    "\n",
    "                for i, c in enumerate(chunks):\n",
    "\n",
    "                    info_splitted.append(\n",
    "                        {\n",
    "                            \"chunk_index\":i,\n",
    "                            \"content\": j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")+\", DOI:\"+j.get(\"DOI\")+\":\\n \"+c,\n",
    "                            \"parent\":key,\n",
    "                            \"split\":True,\n",
    "                            \"DOI\":j.get(\"DOI\"),\n",
    "                            \"Reference\": j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "\n",
    "                info_splitted.append(\n",
    "                        {\n",
    "                            \"chunk_index\":0,\n",
    "                            \"content\":j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")+\", DOI:\"+j.get(\"DOI\")+\":\\n \"+value,\n",
    "                            \"parent\":key,\n",
    "                            \"split\":False,\n",
    "                            \"DOI\":j.get(\"DOI\"),\n",
    "                            \"Reference\": j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0809f73",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065057d",
   "metadata": {},
   "source": [
    "We chose this embedding according to leaderboard of HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bae4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function2 = HuggingFaceEmbeddings(model_name=\"avsolatorio/GIST-small-Embedding-v0\", model_kwargs={'device': 'cuda'}) # model_kwargs={'device': 'cuda'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786e097",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961b72f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info_splitted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. Prepare documents, metadata, and IDs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m texts = [chunk[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43minfo_splitted\u001b[49m]\n\u001b[32m      3\u001b[39m metadatas = [{\u001b[33m\"\u001b[39m\u001b[33mparent\u001b[39m\u001b[33m\"\u001b[39m: chunk[\u001b[33m\"\u001b[39m\u001b[33mparent\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m: chunk[\u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m],\u001b[33m\"\u001b[39m\u001b[33mDOI\u001b[39m\u001b[33m\"\u001b[39m: chunk[\u001b[33m\"\u001b[39m\u001b[33mDOI\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mReference\u001b[39m\u001b[33m\"\u001b[39m: chunk[\u001b[33m\"\u001b[39m\u001b[33mReference\u001b[39m\u001b[33m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m info_splitted]\n\u001b[32m      4\u001b[39m ids = [\u001b[38;5;28mstr\u001b[39m(uuid.uuid1()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m metadatas]\n",
      "\u001b[31mNameError\u001b[39m: name 'info_splitted' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Prepare documents, metadata, and IDs\n",
    "texts = [chunk[\"content\"] for chunk in info_splitted]\n",
    "metadatas = [{\"parent\": chunk[\"parent\"], \"chunk_index\": chunk[\"chunk_index\"],\"DOI\": chunk[\"DOI\"], \"Reference\": chunk[\"Reference\"]} for chunk in info_splitted]\n",
    "ids = [str(uuid.uuid1()) for _ in metadatas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b79161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_function2,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids,\n",
    "    collection_name=\"ReproRAG\",\n",
    "    persist_directory=\"./chromaRepro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cb121",
   "metadata": {},
   "source": [
    "We check that the search works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b92eabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_index': 10, 'Reference': 'P. Diaz-Gimeno et al.,2022', 'DOI': 'https://doi.org/10.1093/humrep/deab262', 'parent': 'Results'}, page_content='P. Diaz-Gimeno et al.,2022, DOI:https://doi.org/10.1093/humrep/deab262\\nPredictive performance comparison of signatures and consistency across endometrial datasets'),\n",
       " Document(metadata={'DOI': 'https://doi.org/10.1016/j.fertnstert.2024.03.015', 'parent': 'Abstract', 'Reference': 'Patricia Diaz-Gimeno et al.,2024', 'chunk_index': 0}, page_content='Patricia Diaz-Gimeno et al.,2024, DOI:https://doi.org/10.1016/j.fertnstert.2024.03.015\\nObjective: To propose a new gene expression signature that identifies endometrial disruptions independent of endometrial luteal phase timing and predicts if patients are at risk of endometrial failure. Design: Multicentric, prospective study. Setting: Reproductive medicine research department in a public hospital affiliated with private fertility clinics and a reproductive genetics laboratory. Patients: Caucasian women (n1⁄4 281; 39.4/C6 4.8 years old with a body mass index of 22.9/C6 3.5 kg/m 2) undergoing hormone replacement therapy between July 2018 and July 2021. Endometrial samples from 217 patients met RNA quality criteria for signature discovery and analysis. Intervention(s): Endometrial biopsies collected in the mid-secretory phase. Main Outcome Measure(s):Endometrial luteal phase timing-corrected expression of 404 genes and reproductive outcomes of thefirst single embryo transfer (SET) after biopsy collection to identify prognostic biomarkers of endometrial failure')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"Is there a signature to predict endometrial disruption?\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c90458",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceba79f",
   "metadata": {},
   "source": [
    "### Agents for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9c2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(llm, prompt):\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c74c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "You are given a piece of scientific text (context).\n",
    "Your task is to generate ONE question and ONE answer from it.\n",
    "\n",
    "Guidelines for the question:\n",
    "- It must be factual and answerable using the context only.\n",
    "- Phrase it naturally, as if a researcher typed it into a search engine.\n",
    "- Do NOT mention \"context\", \"passage\", or \"according to the text\".\n",
    "- The question should be specific and concise.\n",
    "\n",
    "Guidelines for the answer:\n",
    "- The answer must be a short, factual statement directly supported by the context.\n",
    "- Do not add explanations, speculation, or references to the text.\n",
    "\n",
    "Formatting rules (strict):\n",
    "Output:::\n",
    "Question: <your question here>\n",
    "Answer: <your answer here>\n",
    "\n",
    "Now here is the context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Output:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "787c51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_splitted_evaluation = [d for d in info_splitted if d['parent'] in ['Abstract','Introduction','Results','Conclusion','Discussion','Methods']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "69dd44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_index': 1, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n . The search identified experiments involving human endometrial transcriptomic case versus control raw data related to uterine pathologies and implantation alterations. The keywords employed in the search included endometriosis, endometrial adenocarcinoma (ADC), recurrent implantation failure (RIF), and recurrent pregnancy loss (RPL), among others (Supplemental Table 1A, available online, for a full list of search terms). No restrictions were placed on publication date or language. Uterine leiomyoma, adenomyosis, and uterine leiomyosarcoma data were not included due to a lack of suitable studies meeting our criteria. For each sample cohort belonging to the same individual study, 39 variables were evaluated (see Supplemental Table 1B), including clinical characteristics of the participants (e.g., age and body mass index), experimental design (e.g., endometrial biopsy preprocessing procedure and transcriptomic platform), and additional study information (e.g', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 0, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n MATERIALS AND METHODS This is a retrospective study integrating case and control data from multiple cohorts with endometrium gene expression in women with uterine disorders. The raw gene expression data and patient meta-data were downloaded from the National Center for Biotechnology Information (NCBI) functional genomics data repository Gene Expression Omnibus (GEO), where data are made available for the scientific community. Following GEO policies, in this repository patient data are anonymized and encrypted, and no additional institutional review board is required for downloading . Study Search and Selection Systematic searches and reviews of individual studies were conducted between October 2016 and June 2017 at the NCBI functional genomics data repository GEO  according to preferred reporting items for systematic reviews and metaanalysis (PRISMA) guidelines', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 1, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n . The search identified experiments involving human endometrial transcriptomic case versus control raw data related to uterine pathologies and implantation alterations. The keywords employed in the search included endometriosis, endometrial adenocarcinoma (ADC), recurrent implantation failure (RIF), and recurrent pregnancy loss (RPL), among others (Supplemental Table 1A, available online, for a full list of search terms). No restrictions were placed on publication date or language. Uterine leiomyoma, adenomyosis, and uterine leiomyosarcoma data were not included due to a lack of suitable studies meeting our criteria. For each sample cohort belonging to the same individual study, 39 variables were evaluated (see Supplemental Table 1B), including clinical characteristics of the participants (e.g., age and body mass index), experimental design (e.g., endometrial biopsy preprocessing procedure and transcriptomic platform), and additional study information (e.g', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 2, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n .g., GEO identifier and associated publication). Thefinal inclusion criteria were that at least one uterine disorder was evaluated in the sample cohort study design, RNA was extracted directly from human endometrial biopsies (samples collected from eutopic endometrium in endometriosis patients), the sample size was greater than three for both case and control groups belonging to the same study, microarray or RNA sequencing data were obtained using Affymetrix, Illumina, or Agilent gene expression platforms, and raw gene expression data were made freely available to download from GEO. Individual Transcriptomic Analysis: Making Functional Results Comparable The same transcriptomic analysis procedure was applied independently to each of the selected individual data sets for making the functional results comparable to one another before being integrated through functional meta-analysis', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 3, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n . The individual transcriptomic analysis consisted of three consecutive steps run in R software  and applied separately to compare cases versus controls within samples from the same study to avoid variability among sample cohorts related to different uterine disorders. The same preprocessing of raw gene expression data, differential expression analysis, and functional enrichment analysis was performed in a sample cohort comparing cases versus controls ( Supplemental Fig. 1A , available online). The preprocessing step was performed using the affy v.1.52.0  R package for studies using Affymetrix platforms to measure gene expression and thelimma v.3.34.5 1262 VOL. 113 NO. 6 / JUNE 2020 ORIGINAL ARTICLE: GENETICS   R package for the remaining platforms. Normalization between samples was done using quantile normalization (limma R package) . Annotation from probe set to gene symbol was implemented with the biomaRt R package v.2.30.0', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 4, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n . Annotation from probe set to gene symbol was implemented with the biomaRt R package v.2.30.0 . Next, exploratory analyses were performed to detect outliers and batch effects. The effect of the cycle phase of endometrial biopsy collection on the data was removed using linear models (limma v.3.34.5) as previously described elsewhere  because the samples were collected throughout the menstrual cycle . This correction ensures that the functional alterations obtained are due to the uterine disorder instead of the different proportions of cell types (stromal, epithelial) and/or the distinct menstrual cycle stages in which the endometrial biopsy samples were obtained . Differential expression analyses were applied using the limma R package  and functional enrichment analyses were executed using the gene set enrichment analysis (GSEA) logistic model implemented in themdgsa R package . TheP values for each function were calculated and corrected for false discovery rate (FDR)', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 5, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n . TheP values for each function were calculated and corrected for false discovery rate (FDR) . Two functional databases were consulted: the Kyoto Encyclopedia of Genes and Genomes (KEGG)  (release 81.0/2017-1-1) and Gene Ontology (GO)  (Release 1.2/2017-03-31). Functional Integration A functional meta-analysis using mdgsa  and metafor , R packages  was applied for each gene function evaluated, integrating each function among the different patient cohorts belonging to the same disorder (see Supplemental Fig. 1A). This methodology robustly highlights the common alterations across a set of studies, thus discarding those alterations that are exclusive of each individual study’s characteristics . In addition, it increases the statistical power through the integration of a higher number of samples and overcomes the limitations of individual studies’ heterogeneity regarding their distinct clinical population characteristics and experimental procedures', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 6, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n . All integrations were performed under the DerSimonian and Laird  random effects model to account for individual study heterogeneity in the R environment . This model weighs the functional results of each study according to their inherent variability measured by the standard deviation (SD) of the logarithm of odds ratio (LOR) obtained in the GSEA analysis for each evaluated function. Consequently, studies with less variability are weighted more in the computation of thefinal meta-analysis results than those with high variability. Thus, meta-analysis highlights the most robust endometrial functions associated with each uterine disorder. A total of 6,390 functions (226 KEGG pathways, 4,405 GO biological processes (BP) terms, 1,093 GO molecular functions (MF) terms, and 666 GO cellular components (CC) terms) were evaluated through meta-analysis for each uterine disorder', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "{'chunk_index': 7, 'content': 'Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\\n .P values were corrected for multiple testing by FDR , and functions were considered statistically signi ficant in the meta-analysis when FDR<0.05. The P values, summary LOR, and 95% confidence interval (CI) were calculated for all functions evaluated in the metaanalysis. To ensure unbiased results, funnel plots were analyzed for each function evaluated in the meta-analysis. The standard error (SE) of the individual LOR for each integrated study was calculated as an accuracy measure of the study performance. The LOR represents the effect size of the function associated with each uterine disorder compared with controls. If study heterogeneity and publication bias did not affect the functional meta-analysis, the individual LOR of all the integrated studies will be inside the 95% CI area of the funnel plot .', 'parent': 'Methods', 'split': True, 'DOI': 'https://doi.org/10.1016/j.fertnstert.2020.01.025', 'Reference': 'Almudena Devesa-Peiro et al.,2020'}\n",
      "Almudena Devesa-Peiro et al.,2020, DOI:https://doi.org/10.1016/j.fertnstert.2020.01.025:\n",
      " MATERIALS AND METHODS This is a retrospective study integrating case and control data from multiple cohorts with endometrium gene expression in women with uterine disorders. The raw gene expression data and patient meta-data were downloaded from the National Center for Biotechnology Information (NCBI) functional genomics data repository Gene Expression Omnibus (GEO), where data are made available for the scientific community. Following GEO policies, in this repository patient data are anonymized and encrypted, and no additional institutional review board is required for downloading . Study Search and Selection Systematic searches and reviews of individual studies were conducted between October 2016 and June 2017 at the NCBI functional genomics data repository GEO  according to preferred reporting items for systematic reviews and metaanalysis (PRISMA) guidelines. The search identified experiments involving human endometrial transcriptomic case versus control raw data related to uterine pathologies and implantation alterations. The keywords employed in the search included endometriosis, endometrial adenocarcinoma (ADC), recurrent implantation failure (RIF), and recurrent pregnancy loss (RPL), among others (Supplemental Table 1A, available online, for a full list of search terms). No restrictions were placed on publication date or language. Uterine leiomyoma, adenomyosis, and uterine leiomyosarcoma data were not included due to a lack of suitable studies meeting our criteria. For each sample cohort belonging to the same individual study, 39 variables were evaluated (see Supplemental Table 1B), including clinical characteristics of the participants (e.g., age and body mass index), experimental design (e.g., endometrial biopsy preprocessing procedure and transcriptomic platform), and additional study information (e.g.g., GEO identifier and associated publication). Thefinal inclusion criteria were that at least one uterine disorder was evaluated in the sample cohort study design, RNA was extracted directly from human endometrial biopsies (samples collected from eutopic endometrium in endometriosis patients), the sample size was greater than three for both case and control groups belonging to the same study, microarray or RNA sequencing data were obtained using Affymetrix, Illumina, or Agilent gene expression platforms, and raw gene expression data were made freely available to download from GEO. Individual Transcriptomic Analysis: Making Functional Results Comparable The same transcriptomic analysis procedure was applied independently to each of the selected individual data sets for making the functional results comparable to one another before being integrated through functional meta-analysis. The individual transcriptomic analysis consisted of three consecutive steps run in R software  and applied separately to compare cases versus controls within samples from the same study to avoid variability among sample cohorts related to different uterine disorders. The same preprocessing of raw gene expression data, differential expression analysis, and functional enrichment analysis was performed in a sample cohort comparing cases versus controls ( Supplemental Fig. 1A , available online). The preprocessing step was performed using the affy v.1.52.0  R package for studies using Affymetrix platforms to measure gene expression and thelimma v.3.34.5 1262 VOL. 113 NO. 6 / JUNE 2020 ORIGINAL ARTICLE: GENETICS   R package for the remaining platforms. Normalization between samples was done using quantile normalization (limma R package) . Annotation from probe set to gene symbol was implemented with the biomaRt R package v.2.30.0. Annotation from probe set to gene symbol was implemented with the biomaRt R package v.2.30.0 . Next, exploratory analyses were performed to detect outliers and batch effects. The effect of the cycle phase of endometrial biopsy collection on the data was removed using linear models (limma v.3.34.5) as previously described elsewhere  because the samples were collected throughout the menstrual cycle . This correction ensures that the functional alterations obtained are due to the uterine disorder instead of the different proportions of cell types (stromal, epithelial) and/or the distinct menstrual cycle stages in which the endometrial biopsy samples were obtained . Differential expression analyses were applied using the limma R package  and functional enrichment analyses were executed using the gene set enrichment analysis (GSEA) logistic model implemented in themdgsa R package . TheP values for each function were calculated and corrected for false discovery rate (FDR). TheP values for each function were calculated and corrected for false discovery rate (FDR) . Two functional databases were consulted: the Kyoto Encyclopedia of Genes and Genomes (KEGG)  (release 81.0/2017-1-1) and Gene Ontology (GO)  (Release 1.2/2017-03-31). Functional Integration A functional meta-analysis using mdgsa  and metafor , R packages  was applied for each gene function evaluated, integrating each function among the different patient cohorts belonging to the same disorder (see Supplemental Fig. 1A). This methodology robustly highlights the common alterations across a set of studies, thus discarding those alterations that are exclusive of each individual study’s characteristics . In addition, it increases the statistical power through the integration of a higher number of samples and overcomes the limitations of individual studies’ heterogeneity regarding their distinct clinical population characteristics and experimental procedures. All integrations were performed under the DerSimonian and Laird  random effects model to account for individual study heterogeneity in the R environment . This model weighs the functional results of each study according to their inherent variability measured by the standard deviation (SD) of the logarithm of odds ratio (LOR) obtained in the GSEA analysis for each evaluated function. Consequently, studies with less variability are weighted more in the computation of thefinal meta-analysis results than those with high variability. Thus, meta-analysis highlights the most robust endometrial functions associated with each uterine disorder. A total of 6,390 functions (226 KEGG pathways, 4,405 GO biological processes (BP) terms, 1,093 GO molecular functions (MF) terms, and 666 GO cellular components (CC) terms) were evaluated through meta-analysis for each uterine disorder.P values were corrected for multiple testing by FDR , and functions were considered statistically signi ficant in the meta-analysis when FDR<0.05. The P values, summary LOR, and 95% confidence interval (CI) were calculated for all functions evaluated in the metaanalysis. To ensure unbiased results, funnel plots were analyzed for each function evaluated in the meta-analysis. The standard error (SE) of the individual LOR for each integrated study was calculated as an accuracy measure of the study performance. The LOR represents the effect size of the function associated with each uterine disorder compared with controls. If study heterogeneity and publication bias did not affect the functional meta-analysis, the individual LOR of all the integrated studies will be inside the 95% CI area of the funnel plot .\n"
     ]
    }
   ],
   "source": [
    "mynumb=10\n",
    "\n",
    "print(info_splitted_evaluation[mynumb])\n",
    "\n",
    "for d in info_splitted_evaluation:\n",
    "    \n",
    "    if d['Reference'] == info_splitted_evaluation[mynumb]['Reference'] and d['parent'] ==  info_splitted_evaluation[mynumb]['parent']:\n",
    "\n",
    "        print(d)\n",
    "\n",
    "        if d['chunk_index'] == 0:\n",
    "            info = d['content']\n",
    "        else:\n",
    "            info+=d['content'].split(\":\\n \")[-1]\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e93f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(piece_of_paper, all_papers):\n",
    "\n",
    "    for d in all_papers:\n",
    "    \n",
    "        if d['Reference'] == piece_of_paper['Reference'] and d['parent'] ==  piece_of_paper['parent']:\n",
    "\n",
    "            if d['chunk_index'] == 0:\n",
    "                info = d['content']\n",
    "            else:\n",
    "                info+=d['content'].split(\":\\n \")[-1]\n",
    "\n",
    "    return(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed9c7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [48:13<00:00,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 600\n",
    "examples = []\n",
    "for sample in tqdm(random.sample(info_splitted_evaluation,N), total=N):\n",
    "    context = get_context(piece_of_paper=sample, all_papers=info_splitted_evaluation)\n",
    "    response = call_llm(llm=llm,prompt=QA_generation_prompt.format(context=context))\n",
    "    \n",
    "    try:\n",
    "        question = response.split(\"Question:\")[-1].split(\"Answer: \")[0].strip()\n",
    "        answer = response.split(\"Answer: \")[-1].strip()\n",
    "        examples.append({\n",
    "            \"context\" : context,\n",
    "            \"question\" : question,\n",
    "            \"answer\" : answer\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "with open(\"examples_evaluation_gemini_2-5.pkl\",\"wb\") as f:\n",
    "    pkl.dump(examples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1d753",
   "metadata": {},
   "source": [
    "## Evaluation of questions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe8cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to researchers in the reproductive medicine field.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d199ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [56:53<00:00,  5.69s/it]  \n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(examples,total=len(examples)):\n",
    "\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a312109",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(examples)\n",
    "generated_questions.loc[:,[\"question\",\"context\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\"]]\n",
    "with open(\"generated_questions_gemini_2-5.pkl\",\"wb\") as f:\n",
    "    pkl.dump(generated_questions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02a98086",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_final = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "with open(\"generated_questions_final_gemini_2-5.pkl\",\"wb\") as f:\n",
    "    pkl.dump(generated_questions_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7651e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_questions_final.pkl\",\"rb\") as f:\n",
    "    generated_questions_final = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15684535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the FGB rs1800790A allele affect fibr...</td>\n",
       "      <td>In F13A 34Val/Val wildtypes, carriage of the F...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>For which patient group might the ERA test be ...</td>\n",
       "      <td>The ERA test may be helpful for women with sus...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What kind of values does the Color Pathway too...</td>\n",
       "      <td>The Color Pathway tool accepts numerical values.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What is the implantation potential of an euplo...</td>\n",
       "      <td>Once an euploid blastocyst is identified, its ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What does the PRISMA 2020 statement reflect?</td>\n",
       "      <td>The PRISMA 2020 statement reflects advances in...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Which genes share genetic susceptibility for A...</td>\n",
       "      <td>The ESR1, HK3, and BRSK1 genes share genetic s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>What database were the GSE26787 and GSE63901 d...</td>\n",
       "      <td>The Gene Expression Omnibus (GEO) database.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>What percentage of women globally are affected...</td>\n",
       "      <td>3.7% of women globally.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>What is the purpose of unique molecular identi...</td>\n",
       "      <td>Unique molecular identifiers are applied to ov...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>What are pinopodes?</td>\n",
       "      <td>Pinopodes are apical protrusions on the epithe...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "6    How does the FGB rs1800790A allele affect fibr...   \n",
       "20   For which patient group might the ERA test be ...   \n",
       "25   What kind of values does the Color Pathway too...   \n",
       "32   What is the implantation potential of an euplo...   \n",
       "38        What does the PRISMA 2020 statement reflect?   \n",
       "..                                                 ...   \n",
       "566  Which genes share genetic susceptibility for A...   \n",
       "573  What database were the GSE26787 and GSE63901 d...   \n",
       "576  What percentage of women globally are affected...   \n",
       "589  What is the purpose of unique molecular identi...   \n",
       "596                                What are pinopodes?   \n",
       "\n",
       "                                                answer  groundedness_score  \\\n",
       "6    In F13A 34Val/Val wildtypes, carriage of the F...                 5.0   \n",
       "20   The ERA test may be helpful for women with sus...                 5.0   \n",
       "25    The Color Pathway tool accepts numerical values.                 5.0   \n",
       "32   Once an euploid blastocyst is identified, its ...                 5.0   \n",
       "38   The PRISMA 2020 statement reflects advances in...                 5.0   \n",
       "..                                                 ...                 ...   \n",
       "566  The ESR1, HK3, and BRSK1 genes share genetic s...                 5.0   \n",
       "573        The Gene Expression Omnibus (GEO) database.                 5.0   \n",
       "576                            3.7% of women globally.                 5.0   \n",
       "589  Unique molecular identifiers are applied to ov...                 5.0   \n",
       "596  Pinopodes are apical protrusions on the epithe...                 5.0   \n",
       "\n",
       "     relevance_score  standalone_score  \n",
       "6                5.0               5.0  \n",
       "20               5.0               5.0  \n",
       "25               4.0               5.0  \n",
       "32               5.0               5.0  \n",
       "38               5.0               5.0  \n",
       "..               ...               ...  \n",
       "566              5.0               5.0  \n",
       "573              4.0               5.0  \n",
       "576              5.0               5.0  \n",
       "589              5.0               5.0  \n",
       "596              4.0               5.0  \n",
       "\n",
       "[70 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_questions_final.loc[:,[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69babbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = generated_questions_final.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d2a80",
   "metadata": {},
   "source": [
    "### Create a function to check RAG performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "878aa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_reader = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\",temperature=0.5,max_output_tokens=1024) # gemma-3-27b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c5ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(\n",
    "    documents,\n",
    "    chunk_size: int,\n",
    "    embedding_model\n",
    "):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_size//10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]  # smart splitting\n",
    "    )\n",
    "\n",
    "    info_splitted = []\n",
    "\n",
    "    for j in documents:\n",
    "\n",
    "        for key, value in j.items():\n",
    "        \n",
    "            if key in ['Abstract', 'Introduction', 'Methods', 'Results', 'Discussion', 'Conclusion',] and value:\n",
    "\n",
    "                if len(value) > 1200:\n",
    "                    chunks = splitter.split_text(value)\n",
    "\n",
    "                    for i, c in enumerate(chunks):\n",
    "\n",
    "                        info_splitted.append(\n",
    "                            {\n",
    "                                \"chunk_index\":i,\n",
    "                                \"content\": j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")+\", DOI:\"+j.get(\"DOI\")+\"\\n\"+c,\n",
    "                                \"parent\":key,\n",
    "                                \"split\":True,\n",
    "                                \"DOI\":j.get(\"DOI\"),\n",
    "                                \"Reference\": j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")\n",
    "                            }\n",
    "                        )\n",
    "                else:\n",
    "\n",
    "                    info_splitted.append(\n",
    "                            {\n",
    "                                \"chunk_index\":0,\n",
    "                                \"content\":j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")+\", DOI:\"+j.get(\"DOI\")+\"\\n\"+value,\n",
    "                                \"parent\":key,\n",
    "                                \"split\":False,\n",
    "                                \"DOI\":j.get(\"DOI\"),\n",
    "                                \"Reference\": j.get('Authors').split(\",\")[0]+\" et al.,\"+j.get('Publication',\"Not identified\")\n",
    "                            }\n",
    "                        )\n",
    "    \n",
    "    texts = [chunk[\"content\"] for chunk in info_splitted]\n",
    "    metadatas = [{\"parent\": chunk[\"parent\"], \"chunk_index\": chunk[\"chunk_index\"],\"DOI\": chunk[\"DOI\"], \"Reference\": chunk[\"Reference\"]} for chunk in info_splitted]\n",
    "    ids = [str(uuid.uuid1()) for _ in metadatas]\n",
    "\n",
    "    db = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids,\n",
    ")\n",
    "\n",
    "    return(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "064d4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e054c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doi_links(text):\n",
    "    \"\"\"\n",
    "    Replace problematic Unicode dashes (like non-breaking hyphen) with normal ASCII dashes.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[\\u2010-\\u2015\\u2212]\", \"-\", text)\n",
    "\n",
    "def retrieve_context(question, k, database):\n",
    "    results = database.similarity_search(question, k)\n",
    "    selected_index = []\n",
    "    ideal_chunks = []\n",
    "    meta_selected = []\n",
    "\n",
    "    def is_new_chunk(r, selected_index):\n",
    "        next_chunk = \"_\".join([r[\"parent\"], r[\"Reference\"], str(r[\"chunk_index\"] + 1)])\n",
    "        prev_chunk = \"_\".join([r[\"parent\"], r[\"Reference\"], str(r[\"chunk_index\"] - 1)])\n",
    "        return next_chunk not in selected_index and prev_chunk not in selected_index\n",
    "\n",
    "    for doc in results:\n",
    "        r = doc.metadata\n",
    "\n",
    "        if r[\"parent\"] not in [\"Journal\", \"DOI\"] and is_new_chunk(r, selected_index):\n",
    "            ii = \"_\".join([r[\"parent\"], r[\"Reference\"], str(r[\"chunk_index\"])])\n",
    "            selected_index.append(ii)\n",
    "\n",
    "            candidates = db.get(\n",
    "                where={\"$and\": [{\"Reference\": r[\"Reference\"]}, {\"parent\": r[\"parent\"]}]}\n",
    "            )\n",
    "\n",
    "            max_index = len(candidates[\"metadatas\"]) - 1\n",
    "\n",
    "            meta_selected.append(candidates[\"metadatas\"])\n",
    "            ideal_chunks.append(\n",
    "                [\n",
    "                    doc\n",
    "                    for doc, meta in zip(\n",
    "                        candidates[\"documents\"], candidates[\"metadatas\"]\n",
    "                    )\n",
    "                    if meta[\"chunk_index\"]\n",
    "                    in [\n",
    "                        r[\"chunk_index\"],\n",
    "                        max(r[\"chunk_index\"] - 1, 0),\n",
    "                        min(r[\"chunk_index\"] + 1, max_index),\n",
    "                    ]\n",
    "                ]\n",
    "                )\n",
    "\n",
    "        context = []\n",
    "        for text, meta in zip(ideal_chunks, meta_selected):\n",
    "            if meta:  # Only proceed if meta is not empty\n",
    "                doi = (\n",
    "                    clean_doi_links(meta[0][\"DOI\"])\n",
    "                    if \"DOI\" in meta[0]\n",
    "                    else \"DOI not available\"\n",
    "                )\n",
    "                context.append(\n",
    "                    f\"Summary:\\n\\n{''.join(text)}\\n\\n\"\n",
    "                )\n",
    "\n",
    "    return(context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff0a7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm,\n",
    "    database,\n",
    "    num_docs_final: int = 7,\n",
    "    recursive_chunk = False\n",
    "):\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    \n",
    "    if  recursive_chunk:\n",
    "        relevant_docs = retrieve_context(question=question, database=database,k=num_docs_final)\n",
    "    else:\n",
    "        relevant_docs = database.similarity_search(query=question, k=num_docs_final)\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer.content, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d66498c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_tests(\n",
    "    eval_dataset,\n",
    "    llm,\n",
    "    database,\n",
    "    output_file,\n",
    "    verbose=False,\n",
    "    test_settings = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, database,recursive_chunk=True)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb5f5681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['instruction', 'reference_answer', 'response'], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a fair evaluator language model.', additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['instruction', 'reference_answer', 'response'], input_types={}, partial_variables={}, template='###Task Description:\\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\\n3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"\\n4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\\n\\n###The instruction to evaluate:\\n{instruction}\\n\\n###Response to evaluate:\\n{response}\\n\\n###Reference Answer (Score 5):\\n{reference_answer}\\n\\n###Score Rubrics:\\n[Is the response correct, accurate, and factual based on the reference answer?]\\nScore 1: The response is completely incorrect, inaccurate, and/or not factual.\\nScore 2: The response is mostly incorrect, inaccurate, and/or not factual.\\nScore 3: The response is somewhat correct, accurate, and/or factual.\\nScore 4: The response is mostly correct, accurate, and factual.\\nScore 5: The response is completely correct, accurate, and factual.\\n\\n###Feedback:'), additional_kwargs={})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19ca2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e159aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b312c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:1500_reader-model:gemini-2.0-flash:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [02:02<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:18<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk:1700_reader-model:gemini-2.0-flash:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:47<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk:1800_reader-model:gemini-2.0-flash:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:41<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n"
     ]
    }
   ],
   "source": [
    "for chunk_size in [1500,1700,1800]:  # Add other chunk sizes (in tokens) as needed\n",
    "        \n",
    "    settings_name = f\"chunk:{chunk_size}_reader-model:gemini-2.0-flash\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "\n",
    "    db = load_embeddings(\n",
    "        info_articles_final,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=embedding_function2,\n",
    "    )\n",
    "\n",
    "    print(\"Running RAG...\")\n",
    "    run_rag_tests(\n",
    "        eval_dataset=eval_dataset,\n",
    "        llm=llm_reader,\n",
    "        database=db,\n",
    "        output_file=output_file_name,\n",
    "        verbose=False,\n",
    "        test_settings=settings_name,\n",
    "    )\n",
    "\n",
    "    print(\"Running evaluation...\")\n",
    "    evaluate_answers(\n",
    "        output_file_name,\n",
    "        llm_reader,\n",
    "        \"gemini-2.0-flash\",\n",
    "        evaluation_prompt_template,\n",
    "    )\n",
    "    print(\"Removing database\")\n",
    "    db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f6b57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23117f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_gemini-2.0-flash\"] = result[\"eval_score_gemini-2.0-flash\"].apply(lambda x: int(x) if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d5ebaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_gemini-2.0-flash\"] = result[\"eval_score_gemini-2.0-flash\"]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e8551da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:1500_reader-model:gemini-2.0-flash.json    0.857143\n",
       "./output/rag_chunk:1700_reader-model:gemini-2.0-flash.json    0.907937\n",
       "./output/rag_chunk:1800_reader-model:gemini-2.0-flash.json    0.907937\n",
       "Name: eval_score_gemini-2.0-flash, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_gemini-2.0-flash\"].mean()\n",
    "average_scores.sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
