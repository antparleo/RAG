{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c7d3cc",
   "metadata": {},
   "source": [
    "# Evaluation of RAG model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758db44",
   "metadata": {},
   "source": [
    "One way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:\n",
    "\n",
    "1. Correctness: Response vs reference answer\n",
    "- Goal: Measure \"how similar/correct is the RAG chain answer, relative to a ground-truth answer\"\n",
    "- Mode: Requires a ground truth (reference) answer supplied through a dataset\n",
    "- Evaluator: Use LLM-as-judge to assess answer correctness.\n",
    "\n",
    "2. Relevance: Response vs input\n",
    "- Goal: Measure \"how well does the generated response address the initial user input\"\n",
    "- Mode: Does not require reference answer, because it will compare the answer to the input question\n",
    "- Evaluator: Use LLM-as-judge to assess answer relevance, helpfulness, etc.\n",
    "\n",
    "3. Groundedness: Response vs retrieved docs\n",
    "- Goal: Measure \"to what extent does the generated response agree with the retrieved context\"\n",
    "- Mode: Does not require reference answer, because it will compare the answer to the retrieved context\n",
    "- Evaluator: Use LLM-as-judge to assess faithfulness, hallucinations, etc.\n",
    "\n",
    "4. Retrieval relevance: Retrieved docs vs input\n",
    "- Goal: Measure \"how relevant are my retrieved results for this query\"\n",
    "- Mode: Does not require reference answer, because it will compare the question to the retrieved context\n",
    "- Evaluator: Use LLM-as-judge to assess relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f50ef5",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf650934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "import os\n",
    "import pickle as pkl\n",
    "import helper_functions as hf\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386d6e5",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa5f239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"api_google.txt\") as f:\n",
    "    api_key = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12de04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = api_key[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71414f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_questions_final_gemini_2.pkl\", \"rb\") as f:\n",
    "    eval_datasets = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef1d5a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac7c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"info_articles_main.pkl\", \"rb\") as f:\n",
    "    info_articles_main = pkl.load(f)\n",
    "with open(\"info_articles_ref_final.pkl\", \"rb\") as f:\n",
    "    info_articles_ref = pkl.load(f)\n",
    "\n",
    "info_articles_final = info_articles_main + info_articles_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2481d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,  # We selected this threshold according to the performance of the model.\n",
    "    chunk_overlap=150,  # 1500/10\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"],  # smart splitting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb9823c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_splitted = hf.chunk_makers(info_articles_final, splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882eb87f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020d1e80",
   "metadata": {},
   "source": [
    "### Agents for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(llm, prompt):\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd25171",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "You are given a piece of scientific text (context).\n",
    "Your task is to generate ONE question and ONE answer from it.\n",
    "\n",
    "Guidelines for the question:\n",
    "- It must be factual and answerable using the context only.\n",
    "- Phrase it naturally, as if a researcher typed it into a search engine.\n",
    "- Do NOT mention \"context\", \"passage\", or \"according to the text\".\n",
    "- The question should be specific and concise.\n",
    "\n",
    "Guidelines for the answer:\n",
    "- The answer must be a short, factual statement directly supported by the context.\n",
    "- Do not add explanations, speculation, or references to the text.\n",
    "\n",
    "Formatting rules (strict):\n",
    "Output:::\n",
    "Question: <your question here>\n",
    "Answer: <your answer here>\n",
    "\n",
    "Now here is the context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Output:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd43aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_splitted_evaluation = [\n",
    "    d\n",
    "    for d in info_splitted\n",
    "    if d[\"parent\"]\n",
    "    in [\"Abstract\", \"Introduction\", \"Results\", \"Conclusion\", \"Discussion\", \"Methods\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(piece_of_paper, all_papers):\n",
    "    for d in all_papers:\n",
    "        if (\n",
    "            d[\"Reference\"] == piece_of_paper[\"Reference\"]\n",
    "            and d[\"parent\"] == piece_of_paper[\"parent\"]\n",
    "        ):\n",
    "            if d[\"chunk_index\"] == 0:\n",
    "                info = d[\"content\"]\n",
    "            else:\n",
    "                info += d[\"content\"].split(\":\\n \")[-1]\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f58c3",
   "metadata": {},
   "source": [
    "We generate 600 questions using the information provided by each article to evaluate the performance of RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [48:13<00:00,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 600\n",
    "examples = []\n",
    "for sample in tqdm(random.sample(info_splitted_evaluation, N), total=N):\n",
    "    context = get_context(piece_of_paper=sample, all_papers=info_splitted_evaluation)\n",
    "    response = call_llm(llm=llm, prompt=QA_generation_prompt.format(context=context))\n",
    "\n",
    "    try:\n",
    "        question = response.split(\"Question:\")[-1].split(\"Answer: \")[0].strip()\n",
    "        answer = response.split(\"Answer: \")[-1].strip()\n",
    "        examples.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "with open(\"examples_evaluation_gemini_2.pkl\", \"wb\") as f:\n",
    "    pkl.dump(examples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a6478",
   "metadata": {},
   "source": [
    "## Evaluation of generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec480d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to researchers in the reproductive medicine field.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcab1d5",
   "metadata": {},
   "source": [
    "#### Evaluating the generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cd19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [56:53<00:00,  5.69s/it]  \n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(examples, total=len(examples)):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm,\n",
    "            question_groundedness_critique_prompt.format(\n",
    "                context=output[\"context\"], question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(examples)\n",
    "generated_questions.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"question\",\n",
    "        \"context\",\n",
    "        \"answer\",\n",
    "        \"groundedness_score\",\n",
    "        \"relevance_score\",\n",
    "        \"standalone_score\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "# Saving the questions with their answers and their scores\n",
    "\n",
    "with open(\"generated_questions_gemini_2.pkl\", \"wb\") as f:\n",
    "    pkl.dump(generated_questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fb481",
   "metadata": {},
   "source": [
    "This evaluation is based on grading. Therefore, only questions with a score of at least 4 in each criterion are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b784799",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_final = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "\n",
    "with open(\"generated_questions_final_gemini_2.pkl\", \"wb\") as f:\n",
    "    pkl.dump(generated_questions_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03052cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the FGB rs1800790A allele affect fibr...</td>\n",
       "      <td>In F13A 34Val/Val wildtypes, carriage of the F...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>For which patient group might the ERA test be ...</td>\n",
       "      <td>The ERA test may be helpful for women with sus...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What kind of values does the Color Pathway too...</td>\n",
       "      <td>The Color Pathway tool accepts numerical values.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What is the implantation potential of an euplo...</td>\n",
       "      <td>Once an euploid blastocyst is identified, its ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What does the PRISMA 2020 statement reflect?</td>\n",
       "      <td>The PRISMA 2020 statement reflects advances in...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Which genes share genetic susceptibility for A...</td>\n",
       "      <td>The ESR1, HK3, and BRSK1 genes share genetic s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>What database were the GSE26787 and GSE63901 d...</td>\n",
       "      <td>The Gene Expression Omnibus (GEO) database.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>What percentage of women globally are affected...</td>\n",
       "      <td>3.7% of women globally.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>What is the purpose of unique molecular identi...</td>\n",
       "      <td>Unique molecular identifiers are applied to ov...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>What are pinopodes?</td>\n",
       "      <td>Pinopodes are apical protrusions on the epithe...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "6    How does the FGB rs1800790A allele affect fibr...   \n",
       "20   For which patient group might the ERA test be ...   \n",
       "25   What kind of values does the Color Pathway too...   \n",
       "32   What is the implantation potential of an euplo...   \n",
       "38        What does the PRISMA 2020 statement reflect?   \n",
       "..                                                 ...   \n",
       "566  Which genes share genetic susceptibility for A...   \n",
       "573  What database were the GSE26787 and GSE63901 d...   \n",
       "576  What percentage of women globally are affected...   \n",
       "589  What is the purpose of unique molecular identi...   \n",
       "596                                What are pinopodes?   \n",
       "\n",
       "                                                answer  groundedness_score  \\\n",
       "6    In F13A 34Val/Val wildtypes, carriage of the F...                 5.0   \n",
       "20   The ERA test may be helpful for women with sus...                 5.0   \n",
       "25    The Color Pathway tool accepts numerical values.                 5.0   \n",
       "32   Once an euploid blastocyst is identified, its ...                 5.0   \n",
       "38   The PRISMA 2020 statement reflects advances in...                 5.0   \n",
       "..                                                 ...                 ...   \n",
       "566  The ESR1, HK3, and BRSK1 genes share genetic s...                 5.0   \n",
       "573        The Gene Expression Omnibus (GEO) database.                 5.0   \n",
       "576                            3.7% of women globally.                 5.0   \n",
       "589  Unique molecular identifiers are applied to ov...                 5.0   \n",
       "596  Pinopodes are apical protrusions on the epithe...                 5.0   \n",
       "\n",
       "     relevance_score  standalone_score  \n",
       "6                5.0               5.0  \n",
       "20               5.0               5.0  \n",
       "25               4.0               5.0  \n",
       "32               5.0               5.0  \n",
       "38               5.0               5.0  \n",
       "..               ...               ...  \n",
       "566              5.0               5.0  \n",
       "573              4.0               5.0  \n",
       "576              5.0               5.0  \n",
       "589              5.0               5.0  \n",
       "596              4.0               5.0  \n",
       "\n",
       "[70 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_questions_final.loc[\n",
    "    :,\n",
    "    [\"question\", \"answer\", \"groundedness_score\", \"relevance_score\", \"standalone_score\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1b70a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have a total of 70 questions'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We turn the df into a dict\n",
    "eval_dataset = generated_questions_final.to_dict(\"records\")\n",
    "f\"We have a total of {len(eval_dataset)} questions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3647da2",
   "metadata": {},
   "source": [
    "### Evaluate RAG performance using grading for correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38e466",
   "metadata": {},
   "source": [
    "##### PROMPTs for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format MUST look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output, it is required.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee924a",
   "metadata": {},
   "source": [
    "### Answering question and getting a grade from 1 to 5 for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5386a96",
   "metadata": {},
   "source": [
    "For each variable, we generate a question and evaluate the correctness using a grading scale from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f21a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_5:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:39<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_6:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_7:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_8:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:42<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_9:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:44<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_10:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:45<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:38<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_4:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:33<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_5:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:36<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_6:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:35<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_7:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:36<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_8:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:36<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:39<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_9:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:36<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_10:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:38<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_4:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:38<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_5:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:38<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:09<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_6:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_7:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:43<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_8:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:42<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_9:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:41<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:43<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_10:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:44<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_4:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:34<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_5:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:37<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_6:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:37<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_7:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:37<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_8:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:38<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_9:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:39<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_10:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_4:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:37<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_5:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:37<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_6:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:38<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_7:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:40<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_8:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:39<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:40<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_9:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:39<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:41<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n",
      "Running evaluation for chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_10:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:42<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing database\n"
     ]
    }
   ],
   "source": [
    "for chunk_size in [1000, 1500, 2000]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for llm_model in [\"gemini-2.0-flash\"]:\n",
    "        llm_reader = init_chat_model(\n",
    "            llm_model,\n",
    "            model_provider=\"google_genai\",\n",
    "            temperature=0.5,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "\n",
    "        for embedding_model in [\n",
    "            \"avsolatorio/GIST-small-Embedding-v0\",\n",
    "            \"Snowflake/snowflake-arctic-embed-s\",\n",
    "            \"intfloat/e5-small-v2\",\n",
    "        ]:\n",
    "            name_model = embedding_model.split(\"/\")[1]\n",
    "            embedding_function = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model, model_kwargs={\"device\": \"cuda\"}\n",
    "            )\n",
    "\n",
    "            for recursive_chunk in [False, True]:\n",
    "                for k in range(4, 11):\n",
    "                    settings_name = f\"chunk_{chunk_size}_reader-model_{llm_model}_emnedding_model_{name_model}recursive_{recursive_chunk}_k_{k}\"\n",
    "                    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "                    if os.path.exists(output_file_name):\n",
    "                        output = pd.DataFrame(json.load(open(output_file_name)))\n",
    "                        if f\"eval_score_{llm_model}\" in output.columns:\n",
    "                            print(\"The file already exists\")\n",
    "                            continue\n",
    "\n",
    "                    print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "                    print(\"Loading knowledge base embeddings...\")\n",
    "\n",
    "                    db = hf.load_embeddings(\n",
    "                        info_articles_final,\n",
    "                        chunk_size=chunk_size,\n",
    "                        embedding_model=embedding_function,\n",
    "                    )\n",
    "\n",
    "                    print(\"Running RAG...\")\n",
    "                    hf.run_rag_tests(\n",
    "                        eval_dataset=eval_dataset,\n",
    "                        llm=llm_reader,\n",
    "                        database=db,\n",
    "                        template=RAG_PROMPT_TEMPLATE,\n",
    "                        output_file=output_file_name,\n",
    "                        verbose=False,\n",
    "                        test_settings=settings_name,\n",
    "                        recursive_chunk=recursive_chunk,\n",
    "                        num_docs_final=k,\n",
    "                    )\n",
    "\n",
    "                    print(\"Running evaluation...\")\n",
    "                    hf.evaluate_answers(\n",
    "                        output_file_name,\n",
    "                        llm_reader,\n",
    "                        llm_model,\n",
    "                        evaluation_prompt_template,\n",
    "                    )\n",
    "                    print(\"Removing database\")\n",
    "                    db.delete_collection()  # We remove the collection in each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e56601",
   "metadata": {},
   "source": [
    "## Evaluation the models for four metrics without grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f0239",
   "metadata": {},
   "source": [
    "We evaluate again the questions generated by using four different metrics correctness, groundedness, relevance and retrieval relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca677d1",
   "metadata": {},
   "source": [
    "- Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00596249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    # Note that the order in the fields are defined is the order in which the model will generate them.\n",
    "    # It is useful to put explanations before responses because it forces the model to think through\n",
    "    # its final response before generating it:\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "\n",
    "# Grade prompt\n",
    "correctness_instructions = \"\"\"You are a senior researcher in reprodutive field and bioinformatics. \n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the PhD Student answer. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the student answer does not contain any conflicting statements.\n",
    "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the PhD student's answer meets all of the criteria.\n",
    "A correctness value of False means that the PhD student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "\n",
    "grader_llm = llm.with_structured_output(\n",
    "    CorrectnessGrade, method=\"json_schema\", strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb56365",
   "metadata": {},
   "source": [
    "- Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d092c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[\n",
    "        bool, ..., \"Provide the score on whether the answer addresses the question\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Grade prompt\n",
    "relevance_instructions = \"\"\"You are a senior researcher in reprodutive field and bioinformatics. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the PhD STUDENT ANSWER is concise and relevant to the QUESTION\n",
    "(2) Ensure the PhD STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the student's answer meets all of the criteria.\n",
    "A relevance value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "relevance_llm = llm.with_structured_output(\n",
    "    RelevanceGrade, method=\"json_schema\", strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c619368",
   "metadata": {},
   "source": [
    "- Retrieval Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67bc4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[\n",
    "        bool,\n",
    "        ...,\n",
    "        \"True if the retrieved documents are relevant to the question, False otherwise\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Grade prompt\n",
    "retrieval_relevance_instructions = \"\"\"You are a senior researcher in reprodutive field and bioinformatics grading PhD students questions. \n",
    "\n",
    "You will be given a QUESTION and a set of FACTS provided by the PhD student. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
    "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
    "(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\n",
    "A relevance value of False means that the FACTS are completely unrelated to the QUESTION.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "retrieval_llm = llm.with_structured_output(\n",
    "    RetrievalRelevanceGrade, method=\"json_schema\", strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb182852",
   "metadata": {},
   "source": [
    "- Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d3f9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[\n",
    "        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Grade prompt\n",
    "grounded_instructions = \"\"\"You are a senior researcher in reprodutive field and bioinformatics grading PhD students questions. \n",
    "\n",
    "You will be given FACTS and a PhD STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the PhD STUDENT ANSWER is grounded in the FACTS. \n",
    "(2) Ensure the PhD STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Grounded:\n",
    "A grounded value of True means that the student's answer meets all of the criteria.\n",
    "A grounded value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grounded_llm = llm.with_structured_output(\n",
    "    GroundedGrade, method=\"json_schema\", strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7966704",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = json.load(open(file, \"r\"))\n",
    "    outputs[re.sub(r\"./output/rag_\", \"\", file)] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407738bd",
   "metadata": {},
   "source": [
    "We load the pkl in case the file had been created but the loop had not finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"./evaluation/evaluation.pkl\", \"rb\") as f:\n",
    "        list_dataframes = pkl.load(f)\n",
    "except Exception:\n",
    "    print(\"File not found\")\n",
    "    list_dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/126 [00:48<1:40:56, 48.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/126 [01:34<1:36:42, 46.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/126 [02:30<1:44:43, 51.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/126 [03:46<2:04:10, 61.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/126 [04:36<1:55:00, 57.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 6/126 [05:22<1:46:16, 53.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7/126 [06:07<1:40:27, 50.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 8/126 [06:55<1:37:38, 49.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9/126 [07:39<1:33:28, 47.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10/126 [08:25<1:31:45, 47.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 11/126 [09:15<1:32:28, 48.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 12/126 [10:01<1:30:06, 47.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 13/126 [10:48<1:29:20, 47.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 14/126 [11:35<1:27:55, 47.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 15/126 [12:21<1:27:01, 47.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 16/126 [13:11<1:27:41, 47.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 17/126 [14:00<1:27:15, 48.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 18/126 [14:48<1:26:41, 48.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 19/126 [15:34<1:24:50, 47.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 20/126 [16:21<1:23:42, 47.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 21/126 [17:10<1:23:23, 47.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 22/126 [17:58<1:22:53, 47.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 23/126 [18:46<1:22:05, 47.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 24/126 [19:33<1:21:03, 47.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 25/126 [20:24<1:21:53, 48.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 26/126 [21:11<1:20:20, 48.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 27/126 [22:00<1:20:03, 48.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 28/126 [22:48<1:19:07, 48.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 29/126 [23:35<1:17:26, 47.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 30/126 [24:23<1:16:28, 47.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 31/126 [25:09<1:15:06, 47.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 32/126 [25:58<1:14:42, 47.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 33/126 [26:48<1:15:13, 48.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 34/126 [27:34<1:13:16, 47.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 35/126 [28:23<1:13:04, 48.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 36/126 [29:08<1:10:52, 47.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 37/126 [29:54<1:09:28, 46.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 38/126 [30:44<1:09:51, 47.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 39/126 [31:33<1:09:49, 48.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 40/126 [32:26<1:10:56, 49.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 41/126 [33:17<1:10:54, 50.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 42/126 [34:09<1:10:48, 50.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 43/126 [34:59<1:09:40, 50.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 44/126 [35:51<1:09:44, 51.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 45/126 [36:39<1:07:22, 49.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 46/126 [37:28<1:06:21, 49.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 47/126 [38:14<1:04:09, 48.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 48/126 [39:02<1:02:47, 48.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 49/126 [39:53<1:03:02, 49.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 50/126 [40:40<1:01:22, 48.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 51/126 [41:31<1:01:45, 49.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 52/126 [42:24<1:02:15, 50.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 53/126 [43:17<1:02:18, 51.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 54/126 [44:09<1:01:38, 51.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 55/126 [44:59<1:00:21, 51.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 56/126 [45:47<58:31, 50.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 57/126 [46:38<57:54, 50.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 58/126 [47:25<55:49, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 59/126 [48:14<55:09, 49.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 60/126 [49:01<53:25, 48.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 61/126 [49:52<53:19, 49.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 62/126 [50:41<52:30, 49.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 63/126 [51:34<52:55, 50.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 64/126 [52:24<52:03, 50.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 65/126 [53:16<51:32, 50.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 66/126 [54:06<50:27, 50.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 67/126 [54:58<50:10, 51.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 68/126 [55:50<49:28, 51.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 69/126 [56:41<48:44, 51.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 70/126 [57:30<47:15, 50.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 71/126 [58:18<45:33, 49.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 72/126 [59:06<44:15, 49.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 73/126 [59:50<42:11, 47.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 74/126 [1:00:41<42:02, 48.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 75/126 [1:01:27<40:49, 48.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 76/126 [1:02:13<39:22, 47.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 77/126 [1:03:02<38:55, 47.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 78/126 [1:03:48<37:48, 47.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 79/126 [1:04:34<36:49, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 80/126 [1:05:25<36:58, 48.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 81/126 [1:06:13<36:03, 48.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 82/126 [1:07:04<35:50, 48.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 83/126 [1:07:51<34:46, 48.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 84/126 [1:08:44<34:48, 49.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 85/126 [1:09:33<33:54, 49.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 86/126 [1:10:24<33:19, 50.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 87/126 [1:11:13<32:12, 49.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 88/126 [1:12:04<31:45, 50.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 89/126 [1:12:54<30:45, 49.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 90/126 [1:13:41<29:27, 49.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 91/126 [1:14:32<29:02, 49.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 92/126 [1:15:22<28:16, 49.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 93/126 [1:16:13<27:35, 50.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 94/126 [1:17:00<26:16, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 95/126 [1:17:50<25:27, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 96/126 [1:18:38<24:32, 49.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 97/126 [1:19:29<23:58, 49.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 98/126 [1:20:17<22:52, 49.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 99/126 [1:21:08<22:18, 49.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 100/126 [1:21:55<21:11, 48.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 101/126 [1:22:43<20:17, 48.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 102/126 [1:23:34<19:40, 49.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 103/126 [1:24:21<18:40, 48.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 104/126 [1:25:11<18:00, 49.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 105/126 [1:26:01<17:15, 49.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 106/126 [1:26:51<16:32, 49.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 107/126 [1:27:39<15:29, 48.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 108/126 [1:28:26<14:34, 48.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 109/126 [1:29:16<13:50, 48.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 110/126 [1:30:04<12:58, 48.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 111/126 [1:30:53<12:12, 48.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 112/126 [1:31:41<11:20, 48.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 113/126 [1:32:31<10:37, 49.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 114/126 [1:33:20<09:45, 48.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 115/126 [1:34:06<08:47, 47.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_True_k_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 116/126 [1:34:55<08:03, 48.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 117/126 [1:35:41<07:09, 47.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_snowflake-arctic-embed-srecursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 118/126 [1:36:28<06:20, 47.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 119/126 [1:37:18<05:37, 48.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 120/126 [1:38:04<04:45, 47.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 121/126 [1:38:53<04:00, 48.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1500_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 122/126 [1:39:41<03:11, 47.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_GIST-small-Embedding-v0recursive_True_k_5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 123/126 [1:40:28<02:22, 47.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 124/126 [1:41:16<01:35, 47.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_1000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_False_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 125/126 [1:42:05<00:48, 48.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk_2000_reader-model_gemini-2.0-flash_emnedding_model_e5-small-v2recursive_True_k_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [1:42:54<00:00, 49.00s/it]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\n",
    "    \"./evaluation/evaluation_log.txt\"\n",
    "):  # We check the file created previously\n",
    "    with open(\"./evaluation/evaluation_log.txt\", \"r\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "else:\n",
    "    lines = []\n",
    "\n",
    "for settings, evaluation in tqdm(outputs.items(), total=len(outputs)):\n",
    "    if settings in lines:\n",
    "        print(f\"Settings: {settings} already analyzed\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Analyzing {settings}\")\n",
    "    aux_correct = []\n",
    "    aux_relevant = []\n",
    "    aux_retrival = []\n",
    "    aux_ground = []\n",
    "\n",
    "    for mydic in evaluation:\n",
    "        aux_correct.append(\n",
    "            hf.correctness(\n",
    "                inputs=mydic,\n",
    "                grader_llm=grader_llm,\n",
    "                correctness_instructions=correctness_instructions,\n",
    "            )\n",
    "        )\n",
    "        aux_relevant.append(\n",
    "            hf.relevance(\n",
    "                inputs=mydic,\n",
    "                relevance_llm=relevance_llm,\n",
    "                relevance_instructions=relevance_instructions,\n",
    "            )\n",
    "        )\n",
    "        aux_retrival.append(\n",
    "            hf.retrieval_relevance(\n",
    "                inputs=mydic,\n",
    "                retrieval_llm=retrieval_llm,\n",
    "                retrieval_relevance_instructions=retrieval_relevance_instructions,\n",
    "            )\n",
    "        )\n",
    "        aux_ground.append(\n",
    "            hf.groundedness(\n",
    "                inputs=mydic,\n",
    "                grounded_llm=grounded_llm,\n",
    "                grounded_instructions=grounded_instructions,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create dataframe\n",
    "    correctness_results = pd.Series(aux_correct, name=\"Correctness\")\n",
    "    relevance_results = pd.Series(aux_relevant, name=\"Relevance\")\n",
    "    retrieval_results = pd.Series(aux_retrival, name=\"Retrieval\")\n",
    "    ground_results = pd.Series(aux_ground, name=\"Groundedness\")\n",
    "    list_dataframes.append(\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.DataFrame.from_dict(evaluation),\n",
    "                relevance_results,\n",
    "                correctness_results,\n",
    "                retrieval_results,\n",
    "                ground_results,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open(\"./evaluation/evaluation.pkl\", \"wb\") as f:\n",
    "        pkl.dump(list_dataframes, f)\n",
    "\n",
    "    with open(\"./evaluation/evaluation_log.txt\", \"a\") as f:\n",
    "        f.write(f\"{settings}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c7ff5",
   "metadata": {},
   "source": [
    "## Evaluate the benchmark (Without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ea5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./evaluation/evaluation_final.pkl\", \"rb\") as f:\n",
    "    list_dataframes = pkl.load(f)\n",
    "with open(\"./generated_questions_final_gemini_2.pkl\", \"rb\") as f:\n",
    "    examples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd66d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does the FGB rs1800790A allele affect fibrinogen levels in F13A 34Val/Val wildtypes?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[\"question\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f09786ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|system|>\n",
    "Give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57046144",
   "metadata": {},
   "source": [
    "We generate each answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c607df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_eval = examples.to_dict(orient=\"records\")\n",
    "llm_reader = init_chat_model(\n",
    "    \"gemini-2.0-flash\",\n",
    "    model_provider=\"google_genai\",\n",
    "    temperature=0.5,\n",
    "    max_output_tokens=1024,\n",
    ")\n",
    "output_file_name = \"./output_bench/benchmark.json\"\n",
    "\n",
    "hf.run_rag_tests(\n",
    "    eval_dataset=examples_eval,\n",
    "    llm=llm_reader,\n",
    "    output_file=output_file_name,\n",
    "    template=template,\n",
    "    rag=False,\n",
    "    verbose=True,\n",
    "    test_settings=\"No_RAG\",\n",
    "    recursive_chunk=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for benchmark\n",
    "with open(\"./output_bench/benchmark.json\", \"rb\") as f:\n",
    "    outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571d605",
   "metadata": {},
   "source": [
    "We apply only those measure that make sense for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78e4ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_benchmark = []\n",
    "aux_correct = []\n",
    "aux_relevant = []\n",
    "\n",
    "for mydic in outputs:\n",
    "    aux_correct.append(\n",
    "        hf.correctness(\n",
    "            inputs=mydic,\n",
    "            grader_llm=grader_llm,\n",
    "            correctness_instructions=correctness_instructions,\n",
    "        )\n",
    "    )\n",
    "    aux_relevant.append(\n",
    "        hf.relevance(\n",
    "            inputs=mydic,\n",
    "            relevance_llm=relevance_llm,\n",
    "            relevance_instructions=relevance_instructions,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create dataframe\n",
    "correctness_results = pd.Series(aux_correct, name=\"Correctness\")\n",
    "relevance_results = pd.Series(aux_relevant, name=\"Relevance\")\n",
    "list_benchmark.append(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame.from_dict(outputs), relevance_results, correctness_results],\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(\"./output_bench/benchmark.pkl\", \"wb\") as f:\n",
    "    pkl.dump(list_benchmark, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
